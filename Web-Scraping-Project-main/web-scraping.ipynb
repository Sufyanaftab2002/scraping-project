{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data= pd.read_csv(\"data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "    Asin = data.Asin[i]\n",
    "    asin = Asin.replace(\"'\", \"\")\n",
    "    Country = data.country[i]\n",
    "    country = Country.replace(\"'\", \"\")\n",
    "    URL = 'https://www.amazon.{}/dp/000{}'.format(country,asin)\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "    page = requests.get(URL, headers=headers)\n",
    "\n",
    "    soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    if(soup1.find(id='productTitle') == None):\n",
    "        continue\n",
    "    \n",
    "    title = soup1.find(id='productTitle').get_text()\n",
    "\n",
    "    if(soup1.find('span', 'a-offscreen') == None):\n",
    "        price = \"Price Unavailable\"\n",
    "    else:\n",
    "        price = soup1.find('span', 'a-offscreen').text\n",
    "\n",
    "    if(soup1.find(id = \"imgBlkFront\")== None):\n",
    "        image = \"Image Unavailable\"\n",
    "    else:\n",
    "        image = soup1.find(id = \"imgBlkFront\")['src']\n",
    "\n",
    "    if (soup1.find(id = \"detailBullets_feature_div\") == None):\n",
    "        details = \"No Details\"\n",
    "    else: \n",
    "        details1 = soup1.find(id = \"detailBullets_feature_div\")\n",
    "        if (details1.ul == None):\n",
    "            details = \"No Details\"\n",
    "        else:\n",
    "            details2 = details1.ul.text\n",
    "            details = \" \".join(details2.split())\n",
    "    \n",
    "    dict = {'Title':title, 'Price':price, 'Image_URL':image, 'Details':details}\n",
    "    \n",
    "    def write_json(new_data, filename='data.json'):\n",
    "        with open(filename,'r+') as file:\n",
    "            # First we load existing data into a dict.\n",
    "            file_data = json.load(file)\n",
    "            # Join new_data with file_data inside emp_details\n",
    "            file_data[\"all_data\"].append(new_data)\n",
    "            # Sets file's current position at offset.\n",
    "            file.seek(0)\n",
    "            # convert back to json.\n",
    "            json.dump(file_data, file, indent = 4)\n",
    "    \n",
    "    write_json(dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c01b336ec49189b2ee4fd00fd700260a439042ec0c501a2a174e4fc71eb009c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
